# S5E11-LGBM-Predicting_Loan_Payback
# Loan Payback Prediction using LightGBM (Kaggle Playground S5E11)

## Problem Statement
The goal of this project is to predict whether a loan will be paid back or defaulted based on customer demographic, financial, and loan-related features.

This project is based on the Kaggle Playground Series Season 5 Episode 11 competition.

---

## Dataset
- Source: Kaggle Playground Series S5E11 â€“ Predicting Loan Payback
- Target variable: `loan_paid_back`
- Data includes a mix of:
  - Numerical features (income, loan amount, credit score, interest rate, etc.)
  - Categorical features (gender, education level, employment status, etc.)

---

## High-Level Approach

The notebook follows a full end-to-end machine learning pipeline:

### 1. Data Preparation
- Load training and test datasets
- Separate target and features
- Combine train and test for consistent feature engineering

### 2. Feature Engineering
The solution focuses heavily on advanced feature engineering techniques inspired by top Kaggle solutions:

- **Categorical combinations**
  - Pairwise combinations of categorical features
- **Count-based features**
  - Frequency counts of categorical values
- **Groupby-based statistics**
  - Aggregations (mean, size) computed using original dataset
- **Target encoding**
  - Applied to high-cardinality categorical features using cross-validation
- **Binning & transformations**
  - Quantile binning
  - Uniform binning
  - Log-transformed binning
- **Digit-level features**
  - Extracting individual digits from numerical variables (e.g. interest rate, credit score)
- **Ratio features**
  - Ratios between training counts and original dataset counts

All engineered features are concatenated and used for model training.

---

## Model Used
- **LightGBM (LGBMClassifier)**
- Chosen because:
  - Performs well on tabular data
  - Handles non-linear relationships efficiently
  - Scales well with large feature sets

### Key Parameters
- Objective: Binary classification
- Metric: AUC
- Cross-validation: 5-Fold KFold
- Regularization applied to control overfitting

---

## Evaluation Strategy
- **Metric:** ROC AUC
- **Why AUC?**
  - Dataset is imbalanced
  - AUC evaluates ranking quality of probability predictions
- Out-of-fold (OOF) predictions used to estimate generalization performance

---

## Training & Validation
- 5-fold cross-validation
- Target encoding applied inside each fold to prevent leakage
- Predictions averaged across folds for final test predictions

---

## Submission
- Final predictions generated by averaging predictions from all folds
- Output formatted as per Kaggle submission requirements

---

## Learning Note
This notebook is based on studying and reproducing a top-performing Kaggle solution.

The purpose of this project is to:
- Understand advanced feature engineering strategies for tabular data
- Learn how cross-validation and target encoding are used in real-world ML problems
- Gain exposure to competition-style machine learning workflows

---

## What I Am Currently Learning from This Project
- How feature engineering impacts model performance more than model choice
- Why LightGBM is effective for structured/tabular datasets
- How to prevent data leakage during target encoding
- How Kaggle-style pipelines are structured end-to-end

---

## Future Improvements
- Experiment with alternative validation strategies
- Reduce feature set using feature importance analysis
- Compare LightGBM with other tree-based models
- Improve computational efficiency of feature generation

